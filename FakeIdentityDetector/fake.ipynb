{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"transaction_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'Index', 'Address'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = [\n",
    "    'Transaction Frequency', 'Average Sent Amount', 'Average Received Amount', \n",
    "    'Unique Sent Addresses', 'Unique Received Addresses', 'Transaction Time Consistency', \n",
    "    'Time Diff between Transactions (Minutes)', 'total Ether sent', 'total ether received', \n",
    "    'Number of Created Contracts', 'Received Tnx', 'min value received', 'avg val sent', \n",
    "    'max val sent', 'avg val received', 'max value received ', 'Sent tnx', 'Avg min between sent tnx',\n",
    "    'total ether balance', 'Total ERC20 tnxs', 'ERC20 avg time between sent tnx', 'ERC20 uniq sent addr', \n",
    "    'ERC20 min val sent', 'ERC20 max val sent', 'ERC20 avg val sent', 'ERC20 avg time between rec tnx', \n",
    "    'ERC20 most sent token type', 'ERC20 total Ether sent contract', 'ERC20 uniq sent token name'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Generate new features from the dataset.\n",
    "    \"\"\"\n",
    "    # Ensure all expected columns are present; fill missing ones with 0.\n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Transaction Frequency\n",
    "    df['Transaction Frequency'] = df['Sent tnx'] + df['Received Tnx']\n",
    "    \n",
    "    # Average Sent and Received Amounts\n",
    "    df['Average Sent Amount'] = df['avg val sent']\n",
    "    df['Average Received Amount'] = df['avg val received']\n",
    "    \n",
    "    # Diversity of Interactions: use alternate column names if available\n",
    "    if 'Unique Sent To Addresses' in df.columns:\n",
    "        df['Unique Sent Addresses'] = df['Unique Sent To Addresses']\n",
    "    elif 'Unique Sent Addresses' not in df.columns:\n",
    "        df['Unique Sent Addresses'] = 0\n",
    "\n",
    "    if 'Unique Received From Addresses' in df.columns:\n",
    "        df['Unique Received Addresses'] = df['Unique Received From Addresses']\n",
    "    elif 'Unique Received Addresses' not in df.columns:\n",
    "        df['Unique Received Addresses'] = 0\n",
    "\n",
    "    # Transaction Time Consistency: use the historical column if it exists\n",
    "    if 'Time Diff between first and last (Mins)' in df.columns:\n",
    "        df['Transaction Time Consistency'] = df['Time Diff between first and last (Mins)']\n",
    "    # Otherwise, assume that column is already provided.\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['FLAG']\n",
    "X = df.drop(columns=['FLAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training feature column order (this is what the pipeline expects)\n",
    "training_columns = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify numerical and categorical columns (from historical data)\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data and Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Machine Learning Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest Accuracy: {:.4f}\".format(test_accuracy))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Prediction Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_features(df_sim):\n",
    "    \"\"\"\n",
    "    Ensure that the simulation DataFrame has exactly the same columns (and order)\n",
    "    as the training data. Missing columns are filled with default value 0.\n",
    "    \"\"\"\n",
    "    for col in training_columns:\n",
    "        if col not in df_sim.columns:\n",
    "            df_sim[col] = 0\n",
    "    # Reorder the columns to match training data\n",
    "    df_sim = df_sim[training_columns]\n",
    "    return df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_real_time_transaction():\n",
    "    \"\"\"\n",
    "    Simulate a single real-time transaction.\n",
    "    \"\"\"\n",
    "    transaction_data = {\n",
    "        'Sent tnx': random.randint(1, 10),\n",
    "        'Received Tnx': random.randint(1, 10),\n",
    "        'avg val sent': random.uniform(0.01, 5.0),\n",
    "        'avg val received': random.uniform(0.01, 5.0),\n",
    "        'Unique Sent To Addresses': random.randint(1, 20),\n",
    "        'Unique Received From Addresses': random.randint(1, 20),\n",
    "        'Time Diff between first and last (Mins)': random.uniform(1, 120),\n",
    "        'Timestamp': datetime.now(),\n",
    "    }\n",
    "    return transaction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_engineering_for_real_time(transaction_data, previous_data=None):\n",
    "    \"\"\"\n",
    "    Generate features for real-time transaction data.\n",
    "    \"\"\"\n",
    "    # Calculate basic features\n",
    "    transaction_frequency = transaction_data['Sent tnx'] + transaction_data['Received Tnx']\n",
    "    average_sent_amount = transaction_data['avg val sent']\n",
    "    average_received_amount = transaction_data['avg val received']\n",
    "    unique_sent_addresses = transaction_data['Unique Sent To Addresses']\n",
    "    unique_received_addresses = transaction_data['Unique Received From Addresses']\n",
    "    time_diff_first_last = transaction_data['Time Diff between first and last (Mins)']\n",
    "    \n",
    "    # Calculate time difference between transactions using previous data (in minutes)\n",
    "    if previous_data is not None:\n",
    "        previous_time = previous_data['Timestamp']\n",
    "        time_diff = (transaction_data['Timestamp'] - previous_time).total_seconds() / 60.0\n",
    "    else:\n",
    "        time_diff = 0\n",
    "\n",
    "    # Build a basic feature dictionary. We only supply a few features,\n",
    "    # and the rest of the columns (expected by the pipeline) will be added with default values.\n",
    "    features = {\n",
    "        'Transaction Frequency': transaction_frequency,\n",
    "        'Average Sent Amount': average_sent_amount,\n",
    "        'Average Received Amount': average_received_amount,\n",
    "        'Unique Sent Addresses': unique_sent_addresses,\n",
    "        'Unique Received Addresses': unique_received_addresses,\n",
    "        'Transaction Time Consistency': time_diff_first_last,  # Using the simulated value\n",
    "        'Time Diff between Transactions (Minutes)': time_diff,\n",
    "        'total Ether sent': 0,  # Dummy value\n",
    "        'total ether received': 0,  # Dummy value\n",
    "        'Number of Created Contracts': 0,  # Dummy value\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and perform feature engineering (this will add any default columns from expected_columns)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    features_df = feature_engineering(features_df)\n",
    "    \n",
    "    # Align the simulation features with the training columns\n",
    "    features_df = align_features(features_df)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_time_monitoring(num_transactions=10, delay=2):\n",
    "    \"\"\"\n",
    "    Simulate real-time monitoring of incoming transactions.\n",
    "    \"\"\"\n",
    "    previous_data = None\n",
    "    print(\"\\n--- Starting Real-Time Transaction Monitoring Simulation ---\")\n",
    "    \n",
    "    for i in range(num_transactions):\n",
    "        transaction_data = simulate_real_time_transaction()\n",
    "        transaction_df = feature_engineering_for_real_time(transaction_data, previous_data)\n",
    "        \n",
    "        # Predict fraud probability using the trained model\n",
    "        fraud_prob = pipeline.predict_proba(transaction_df)[0, 1]\n",
    "        prediction = pipeline.predict(transaction_df)[0]\n",
    "        \n",
    "        print(f\"\\nTransaction {i + 1}:\")\n",
    "        print(f\"Predicted Fraud Probability: {fraud_prob:.4f}\")\n",
    "        \n",
    "        if prediction == 1:\n",
    "            print(\"⚠️ Fraudulent Transaction Detected! Stopping transaction.\")\n",
    "        else:\n",
    "            print(\"Transaction appears legitimate. Proceeding with processing.\")\n",
    "        \n",
    "        previous_data = transaction_data  # Update previous transaction info\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print(\"\\n--- Simulation Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9898\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1533\n",
      "           1       1.00      0.96      0.98       436\n",
      "\n",
      "    accuracy                           0.99      1969\n",
      "   macro avg       0.99      0.98      0.99      1969\n",
      "weighted avg       0.99      0.99      0.99      1969\n",
      "\n",
      "\n",
      "--- Starting Real-Time Transaction Monitoring Simulation ---\n",
      "\n",
      "Transaction 1:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 2:\n",
      "Predicted Fraud Probability: 0.5400\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 3:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 4:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 5:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 6:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 7:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 8:\n",
      "Predicted Fraud Probability: 0.5500\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 9:\n",
      "Predicted Fraud Probability: 0.5400\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "Transaction 10:\n",
      "Predicted Fraud Probability: 0.5300\n",
      "⚠️ Fraudulent Transaction Detected! Stopping transaction.\n",
      "\n",
      "--- Simulation Completed ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run the real-time monitoring simulation\n",
    "real_time_monitoring(num_transactions=10, delay=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fraud_detection_model.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, 'fraud_detection_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_columns.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X.columns.tolist(), 'feature_columns.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
