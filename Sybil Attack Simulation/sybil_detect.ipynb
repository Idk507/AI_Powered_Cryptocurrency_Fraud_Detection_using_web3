{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"transaction_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'Index', 'Address'])\n",
    "\n",
    "# Define target and features\n",
    "y = df['FLAG']\n",
    "X = df.drop(columns=['FLAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FLAG', 'Avg min between sent tnx', 'Avg min between received tnx',\n",
       "       'Time Diff between first and last (Mins)', 'Sent tnx', 'Received Tnx',\n",
       "       'Number of Created Contracts', 'Unique Received From Addresses',\n",
       "       'Unique Sent To Addresses', 'min value received', 'max value received ',\n",
       "       'avg val received', 'min val sent', 'max val sent', 'avg val sent',\n",
       "       'min value sent to contract', 'max val sent to contract',\n",
       "       'avg value sent to contract',\n",
       "       'total transactions (including tnx to create contract',\n",
       "       'total Ether sent', 'total ether received',\n",
       "       'total ether sent contracts', 'total ether balance',\n",
       "       ' Total ERC20 tnxs', ' ERC20 total Ether received',\n",
       "       ' ERC20 total ether sent', ' ERC20 total Ether sent contract',\n",
       "       ' ERC20 uniq sent addr', ' ERC20 uniq rec addr',\n",
       "       ' ERC20 uniq sent addr.1', ' ERC20 uniq rec contract addr',\n",
       "       ' ERC20 avg time between sent tnx', ' ERC20 avg time between rec tnx',\n",
       "       ' ERC20 avg time between rec 2 tnx',\n",
       "       ' ERC20 avg time between contract tnx', ' ERC20 min val rec',\n",
       "       ' ERC20 max val rec', ' ERC20 avg val rec', ' ERC20 min val sent',\n",
       "       ' ERC20 max val sent', ' ERC20 avg val sent',\n",
       "       ' ERC20 min val sent contract', ' ERC20 max val sent contract',\n",
       "       ' ERC20 avg val sent contract', ' ERC20 uniq sent token name',\n",
       "       ' ERC20 uniq rec token name', ' ERC20 most sent token type',\n",
       "       ' ERC20_most_rec_token_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expected_columns = [\n",
    "    'Transaction Frequency', 'Average Sent Amount', 'Average Received Amount',\n",
    "    'Unique Sent Addresses', 'Unique Received Addresses', 'Transaction Time Consistency',\n",
    "    'Time Diff between Transactions (Minutes)', 'Account Age', 'Total Sent Transactions',\n",
    "    'Total Received Transactions', 'Device Fingerprint', 'IP Address', 'Geolocation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Generate new features based on Sybil attack detection requirements.\n",
    "    For raw columns, check for existence before computing derived features.\n",
    "    \"\"\"\n",
    "    # Transaction Frequency and Total Transaction Counts\n",
    "    if 'Sent tnx' in df.columns and 'Received Tnx' in df.columns:\n",
    "        df['Transaction Frequency'] = df['Sent tnx'] + df['Received Tnx']\n",
    "        df['Total Sent Transactions'] = df['Sent tnx']\n",
    "        df['Total Received Transactions'] = df['Received Tnx']\n",
    "    else:\n",
    "        df['Transaction Frequency'] = df.get('Transaction Frequency', 0)\n",
    "        df['Total Sent Transactions'] = df.get('Total Sent Transactions', 0)\n",
    "        df['Total Received Transactions'] = df.get('Total Received Transactions', 0)\n",
    "    \n",
    "    # Average Sent and Received Amounts\n",
    "    if 'avg val sent' in df.columns:\n",
    "        df['Average Sent Amount'] = df['avg val sent']\n",
    "    else:\n",
    "        df['Average Sent Amount'] = df.get('Average Sent Amount', 0)\n",
    "        \n",
    "    if 'avg val received' in df.columns:\n",
    "        df['Average Received Amount'] = df['avg val received']\n",
    "    else:\n",
    "        df['Average Received Amount'] = df.get('Average Received Amount', 0)\n",
    "    \n",
    "    # Unique Addresses\n",
    "    if 'Unique Sent To Addresses' in df.columns:\n",
    "        df['Unique Sent Addresses'] = df['Unique Sent To Addresses']\n",
    "    else:\n",
    "        df['Unique Sent Addresses'] = df.get('Unique Sent Addresses', 0)\n",
    "        \n",
    "    if 'Unique Received From Addresses' in df.columns:\n",
    "        df['Unique Received Addresses'] = df['Unique Received From Addresses']\n",
    "    else:\n",
    "        df['Unique Received Addresses'] = df.get('Unique Received Addresses', 0)\n",
    "    \n",
    "    # Transaction Time Consistency\n",
    "    if 'Time Diff between first and last (Mins)' in df.columns:\n",
    "        df['Transaction Time Consistency'] = df['Time Diff between first and last (Mins)']\n",
    "    else:\n",
    "        df['Transaction Time Consistency'] = df.get('Transaction Time Consistency', 0)\n",
    "    \n",
    "    # Account Age (if not available, default to 0)\n",
    "    if 'Account Age' not in df.columns:\n",
    "        df['Account Age'] = 0\n",
    "    \n",
    "    # Device Fingerprint, IP Address, and Geolocation (simulate if missing)\n",
    "    if 'Device Fingerprint' not in df.columns:\n",
    "        df['Device Fingerprint'] = 'Unknown'\n",
    "    if 'IP Address' not in df.columns:\n",
    "        df['IP Address'] = 'Unknown'\n",
    "    if 'Geolocation' not in df.columns:\n",
    "        df['Geolocation'] = 'Unknown'\n",
    "    \n",
    "    # Time Diff between Transactions (Minutes)\n",
    "    if 'Time Diff between Transactions (Minutes)' not in df.columns:\n",
    "        df['Time Diff between Transactions (Minutes)'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply feature engineering on historical data\n",
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, restrict to the expected features plus the target column.\n",
    "# Assume the historical dataset has a target column named 'FLAG'\n",
    "if 'FLAG' in df.columns:\n",
    "    df = df[expected_columns + ['FLAG']]\n",
    "else:\n",
    "    # For demonstration, if no FLAG exists, simulate it.\n",
    "    df['FLAG'] = np.random.randint(0, 2, size=len(df))\n",
    "    df = df[expected_columns + ['FLAG']]\n",
    "\n",
    "# Define target and features\n",
    "y = df['FLAG']\n",
    "X = df[expected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build and Train the Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify numerical and categorical columns among the expected features\n",
    "numerical_cols = [col for col in expected_columns if df[col].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [col for col in expected_columns if df[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Build the complete pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest Accuracy: {:.4f}\".format(test_accuracy))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Sybil Attack Detection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def align_features(df_sim):\n",
    "    \"\"\"\n",
    "    Ensure that the simulation DataFrame has exactly the same columns (and order)\n",
    "    as defined in expected_columns. Missing columns are filled with default values.\n",
    "    \"\"\"\n",
    "    for col in expected_columns:\n",
    "        if col not in df_sim.columns:\n",
    "            if col in numerical_cols:\n",
    "                df_sim[col] = 0\n",
    "            else:\n",
    "                df_sim[col] = 'Unknown'\n",
    "    df_sim = df_sim[expected_columns]\n",
    "    return df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_real_time_transaction():\n",
    "    \"\"\"\n",
    "    Simulate a single real-time transaction with raw data.\n",
    "    \"\"\"\n",
    "    transaction_data = {\n",
    "        'Sent tnx': random.randint(1, 10),\n",
    "        'Received Tnx': random.randint(1, 10),\n",
    "        'avg val sent': random.uniform(0.01, 5.0),\n",
    "        'avg val received': random.uniform(0.01, 5.0),\n",
    "        'Unique Sent To Addresses': random.randint(1, 20),\n",
    "        'Unique Received From Addresses': random.randint(1, 20),\n",
    "        'Time Diff between first and last (Mins)': random.uniform(1, 120),\n",
    "        'Timestamp': datetime.now(),\n",
    "        'Device Fingerprint': random.choice(['DeviceA', 'DeviceB', 'DeviceC']),\n",
    "        'IP Address': random.choice(['192.168.0.1', '192.168.0.2']),\n",
    "        'Geolocation': random.choice(['US', 'EU', 'Asia']),\n",
    "        'Account Age': random.randint(1, 100)\n",
    "    }\n",
    "    return transaction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def feature_engineering_for_real_time(transaction_data, previous_data=None):\n",
    "    \"\"\"\n",
    "    Generate features for real-time transaction data.\n",
    "    Computes derived features and then aligns the DataFrame with the expected features.\n",
    "    \"\"\"\n",
    "    # Compute derived features from the raw transaction data\n",
    "    transaction_frequency = transaction_data['Sent tnx'] + transaction_data['Received Tnx']\n",
    "    average_sent_amount = transaction_data['avg val sent']\n",
    "    average_received_amount = transaction_data['avg val received']\n",
    "    unique_sent_addresses = transaction_data['Unique Sent To Addresses']\n",
    "    unique_received_addresses = transaction_data['Unique Received From Addresses']\n",
    "    time_diff_first_last = transaction_data['Time Diff between first and last (Mins)']\n",
    "    \n",
    "    # Calculate time difference between transactions (if previous transaction is available)\n",
    "    if previous_data is not None:\n",
    "        previous_time = previous_data['Timestamp']\n",
    "        time_diff = (transaction_data['Timestamp'] - previous_time).total_seconds() / 60.0\n",
    "    else:\n",
    "        time_diff = 0\n",
    "\n",
    "    # Build a dictionary of features to use for prediction\n",
    "    features = {\n",
    "        'Transaction Frequency': transaction_frequency,\n",
    "        'Average Sent Amount': average_sent_amount,\n",
    "        'Average Received Amount': average_received_amount,\n",
    "        'Unique Sent Addresses': unique_sent_addresses,\n",
    "        'Unique Received Addresses': unique_received_addresses,\n",
    "        'Transaction Time Consistency': time_diff_first_last,\n",
    "        'Time Diff between Transactions (Minutes)': time_diff,\n",
    "        'Account Age': transaction_data['Account Age'],\n",
    "        'Total Sent Transactions': transaction_data['Sent tnx'],\n",
    "        'Total Received Transactions': transaction_data['Received Tnx'],\n",
    "        'Device Fingerprint': transaction_data['Device Fingerprint'],\n",
    "        'IP Address': transaction_data['IP Address'],\n",
    "        'Geolocation': transaction_data['Geolocation']\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame and apply feature engineering to fill in any missing derived features\n",
    "    features_df = pd.DataFrame([features])\n",
    "    features_df = feature_engineering(features_df)\n",
    "    # Ensure column alignment with training features\n",
    "    features_df = align_features(features_df)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9340\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1533\n",
      "           1       0.91      0.78      0.84       436\n",
      "\n",
      "    accuracy                           0.93      1969\n",
      "   macro avg       0.93      0.88      0.90      1969\n",
      "weighted avg       0.93      0.93      0.93      1969\n",
      "\n",
      "\n",
      "--- Starting Real-Time Sybil Attack Detection Simulation ---\n",
      "\n",
      "Transaction 1:\n",
      "Predicted Sybil Attack Probability: 0.4900\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 2:\n",
      "Predicted Sybil Attack Probability: 0.3600\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 3:\n",
      "Predicted Sybil Attack Probability: 0.5700\n",
      "⚠️ Sybil Attack Detected! Stopping transaction.\n",
      "\n",
      "Transaction 4:\n",
      "Predicted Sybil Attack Probability: 0.3900\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 5:\n",
      "Predicted Sybil Attack Probability: 0.2100\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 6:\n",
      "Predicted Sybil Attack Probability: 0.2000\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 7:\n",
      "Predicted Sybil Attack Probability: 0.4300\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 8:\n",
      "Predicted Sybil Attack Probability: 0.5600\n",
      "⚠️ Sybil Attack Detected! Stopping transaction.\n",
      "\n",
      "Transaction 9:\n",
      "Predicted Sybil Attack Probability: 0.4300\n",
      "Transaction appears legitimate. Proceeding with processing.\n",
      "\n",
      "Transaction 10:\n",
      "Predicted Sybil Attack Probability: 0.5300\n",
      "⚠️ Sybil Attack Detected! Stopping transaction.\n",
      "\n",
      "--- Simulation Completed ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def real_time_monitoring(num_transactions=10, delay=2):\n",
    "    \"\"\"\n",
    "    Simulate real-time monitoring of incoming transactions for Sybil attack detection.\n",
    "    \"\"\"\n",
    "    previous_data = None  # For computing time differences\n",
    "    print(\"\\n--- Starting Real-Time Sybil Attack Detection Simulation ---\")\n",
    "    \n",
    "    for i in range(num_transactions):\n",
    "        transaction_data = simulate_real_time_transaction()\n",
    "        transaction_df = feature_engineering_for_real_time(transaction_data, previous_data)\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        sybil_prob = pipeline.predict_proba(transaction_df)[0, 1]\n",
    "        prediction = pipeline.predict(transaction_df)[0]\n",
    "        \n",
    "        print(f\"\\nTransaction {i + 1}:\")\n",
    "        print(f\"Predicted Sybil Attack Probability: {sybil_prob:.4f}\")\n",
    "        if prediction == 1:\n",
    "            print(\"⚠️ Sybil Attack Detected! Stopping transaction.\")\n",
    "        else:\n",
    "            print(\"Transaction appears legitimate. Proceeding with processing.\")\n",
    "        \n",
    "        previous_data = transaction_data\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print(\"\\n--- Simulation Completed ---\")\n",
    "\n",
    "# Run the real-time Sybil attack monitoring simulation\n",
    "real_time_monitoring(num_transactions=10, delay=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model.pkl and feature columns saved as feature_columns.pkl!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "training_columns = X.columns.tolist()\n",
    "joblib.dump(training_columns, \"feature_columns.pkl\")\n",
    "print(\"Model saved as model.pkl and feature columns saved as feature_columns.pkl!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "joblib.dump(pipeline, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
